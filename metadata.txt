1. Learning Outcome:
   - Master the end-to-end Deep Learning lifecycle from data ingestion to deployment.
   - Learn to write modular, production-grade Python code for Computer Vision projects.
   - Understand MLOps principles using DVC for pipeline orchestration and MLflow for experiment tracking.
   - Gain hands-on experience with AWS Cloud services (ECR, EC2) for containerized deployment.
   - Implement CI/CD pipelines using GitHub Actions for automated build and deploy.
   - Build and deploy scalable APIs using Flask and Docker to serve model predictions.

2. What you will build:
   - A robust "Kidney Tumor Identification System" capable of accurately classifying kidney CT scans as "Tumor" or "Normal".
   - An automated Training Pipeline that handles data ingestion, base model preparation (VGG16), training, and evaluation.
   - A Prediction Pipeline exposed via a Flask web application for real-time user predictions.
   - A fully automated deployment workflow that builds Docker images and deploys them to AWS EC2 via ECR whenever code is pushed to GitHub.

3. Prerequisite:
   - Proficiency in Python programming.
   - Fundamental understanding of Deep Learning concepts (CNNs, Transfer Learning).
   - Basic familiarity with Git and version control.
   - An AWS account (free tier is sufficient) and basic cloud knowledge is recommended.

4. Technologies:
   - Programming Language: Python
   - Web Framework: Flask
   - Deep Learning Library: TensorFlow (Keras)
   - MLOps & Tracking: DVC (Data Version Control), MLflow
   - Containerization: Docker
   - CI/CD: GitHub Actions
   - Cloud Platform: AWS (ECR, EC2)
   - Libraries: Pandas, NumPy, Matplotlib, Seaborn, Python-box
